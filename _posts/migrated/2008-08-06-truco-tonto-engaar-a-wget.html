---
migrated: node/963
layout: post
title: ! 'Truco tonto: engañar a wget'
created: 1217975009
author: int-0
category: recipe
tags:
- networking
---
Hola majetes! quizá alguna vez hayáis intentado descargar una página completa con <em>wget -r</em> y sin ningún motivo aparente se ha negado a llevar a cabo tal tarea... quizás podáis con este <em>cutre truco</em></blockquote>

<!--break-->



<h2>Quien necesita este truco</h2>
Veamos... suponed que queréis descargar todo un sitio <b>completo</b> con <em>wget</em>. El sitio detecta que intentáis conectar con un navegador <em>extraño</em> y os deniega las páginas. Cambiáis el <em>user agent</em> del <em>wget</em> con la opción adecuada... ¡y tampoco funciona!... pero esta vez habéis obtenido un fichero más: <b>robot.txt</b>. Este es un fichero que envían los servidores a los <em>robots</em> webs (<em>spiders</em>) con información sobre qué pueden y qué no pueden descargar (por ejemplo para evitar que un <em>spider</em> se quede mandando peticiones <em>php</em> sin fin).

Dado que este <tt>robots.txt</tt> se supone que es para hacerle caso, <em>wget</em> se compromete a usarlo y no ofrece ninguna opción para ignorarlo... (en la página <em>man</em> te vienen a decir que es compromiso de los desarrolladores) así que si la página que quieres descargar manda un <tt>robots.txt</tt> como el siguiente:
<pre>User-Agent: *
Disallow: /</pre>

Pues ya la hemos liado porque nuestro <em>wget</em> va a negarse a descargar nada...

<h2>El cutre truco</h2>
Vale... supongamos que queremos descargar completamente el dominio <em><a href="http://www.listillos.com">www.listillos.com</a></em> y al intentarlo hemos obtenido un directorio <tt>\<a href="http://www.listillos.com">www.listillos.com</a></tt> dentro del cual tenemos el temido archivo <tt>robots.txt</tt> que comentábamos anteriormente. Pues bien, si al <em>wget</em> le damos la opción <b>-c</b> (<em>continue</em>), cuando intente descargar <tt>robots.txt</tt> no lo hará porque ya se descargó uno (y será el que use), así pues sólo tendremos que modificar el archivo por algo parecido a:
<pre>User-Agent: *
Allow: *
Disallow: /nada_de_nada_hombre_que_las_cosas_son_pa_compartirlas</pre>

Es importante la última línea, porque si sólo cambiamos el <em>Disallow</em> por el <em>Allow</em> nuestro <tt>robots.txt</tt> ocupará menos que el que hay en el servidor y entonces <em>wget</em> lo volverá a descargar. Con la última línea inútil final nos aseguramos que <em>wget</em> no se descargue el fichero real... entonces... ¿cuál usa? pues el nuestro :P.

Como veis el truco es tan tonto que casi me da vergüenza ponerlo, pero quizá ayude a alguien... :). De todas formas los autores de <em>wget</em> podrían corregir esto de una forma bastante sencilla: descargando <tt>robots.txt</tt> y procesándolo directamente en memoria, sin necesidad de grabarlo en disco...
