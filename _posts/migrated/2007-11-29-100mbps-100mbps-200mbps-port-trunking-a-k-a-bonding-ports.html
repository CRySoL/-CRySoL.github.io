---
migrated: node/811
layout: post
title: ! '100Mbps + 100Mbps = 200Mbps: port trunking (a.k.a. bonding ports)'
created: 1196355607
author: int-0
category: receta
tags:
- networking
- Arco
---
<blockquote>Probablemente muchos de vosotros tengáis equipos con varias tarjetas de red, esto puede usarse para muchas cosas, una de ellas (a mi entender <em>muy</em> interesante) es el <em>port trunking</em> o también conocido como <em>port bonding</em>.
</blockquote>






<h2>¿Qué es el <em>port bonding</em>?</h2>
Es una técnica que permite agregar varios interfaces de red físicos en uno único virtual. A cada interfaz físico se le denominará <em>slave</em> (esclavo). Con esto podemos realizar un balanceo de carga entre las dos interfaces y conseguir un ancho de banda final igual a la suma de los anchos de banda de cada <em>slave</em>. Además de una ventaja adicional inmediata: redundancia de la conexión. Tenemos varios enlaces físicos a la red, perder alguno de ellos implica una degradación de servicio pero no la pérdida completa de conexión.

<h2>Ingredientes</h2>
Es necesario un módulo que viene de serie incluído en los <em>linux</em> 2.4 y 2.6. Además también hace falta el programa que gestiona las interfaces: <em>ifenslave</em>, que viene empaquetado en <em>Debian</em> para disfrute de todos nosotros:


<div class="console">
{% highlight console %}
# aptitude install ifenslave-2.6
{% endhighlight %}
</div>

<h2>El módulo <em>bonding</em></h2>
Para crear la interfaz virtual debemos cargar un módulo que la gestione aplicando alguna política o algoritmo para ello. Este módulo tiene muchos parámetros, aunque creo que antes de listarlos todos explicaremos un poco el funcionamiento teórico para entender mejor los parámetros.

<h3><em>Slaves</em> y colas</h3>
Toda interfaz de red tiene asignada automáticamente una cola de envío y otra de recepción. Cada <em>slave</em> tiene también sus propias colas. El esquema es más o menos el siguiente (supondremos dos <em>slaves</em>):
<pre>
RED ----> ETH0(RX) --->\
                        BOND0(RX) ----> S.O.
RED ----> ETH1(RX) --->/

                    /----> ETH0(TX) ----> RED
S.O. ----> BOND0(TX)
                    \----> ETH1(TX) ----> RED
</pre>

(Ole ese ASCII ART!)
En cuanto a la recepción hay poco que decir, cuando llega un paquete
a algún <em>slave</em> se pasa a la cola de recepción de la interfaz virtual. Si un enlace físico falla, hay poco que podamos hacer: símplemente se degradará el servicio. Pero para el envío hay un poco más de historia, puesto que si detectamos que un enlace se ha perdido, deberemos pasar todos los paquetes de la cola de esa interfaz a la esclava que sigue funcionando para después congelar esa cola de envío degradando así el rendimiento de la interfaz. Por tanto debemos poder detectar cuando un enlace se ha caído, a este respecto existen dos posibilidades: detección por <em>ARP</em> o mediante <em>MII</em> (<em>Media Indepent Interface</em>).
<ul><li>Detección <em>ARP</em>: consiste en enviar periódicamente peticiones <em>ARP</em> por un <em>slave</em>, si estas peticiones fallan el enlace se considera caído.</li>
<li>Detección <em>MII</em>: si el módulo de la <em>NIC</em> (<em>Network Interface Card</em>) implementa ciertas llamadas, se puede detectar cuando un enlace está físicamente caído, en cuyo caso, se deshabilita el <em>slave</em>.</li></ul>
Evidentemente, los esclavos deshabilitados se seguirán probando hasta que vuelvan a estar operativos, momento en el cual se volverán a habilitar dentro de la interfaz virtual.

<h3>Balanceos de carga</h3>
Probablemente este sea el aspecto más importante del <em>bonding</em>: cómo balancear la carga entre todos los esclavos. En este sentido el módulo tiene un parámetro que permite indicar qué algoritmo se debe usar, los algoritmos implementados son:
<ul><li><em>balance-rr</em> (modo 0): se emplea un algoritmo <em>round robin</em> entre la cola virtual y las de los esclavos. Es algo así como: un paquetillo para un esclavo, otro para otro esclavo, un paquetillo para un esclavo, otro para el otro... etc. Es el algoritmo que se usa por defecto.</li>
<li><em>active-backup</em> (modo 1): realmente no balancea la carga, usa sólo un esclavo y en caso de fallar, usa el siguiente disponible.</li>
<li><em>balance-xor</em> (modo 2): emplea una formulita para decidir por qué interfaz sale: (source-MAC xor dest-MAC) mod n-slaves.</li>
<li><em>broadcast</em> (modo 3): se transmite todo por todas las interfaces. Este método no balancea tampoco, pero provee tolerancia a fallos.</li>
<li><em>802.3ad</em> (modo 4): emplea algoritmos definidos en el estándar <em>IEEE 802.3ad</em>.</li>
<li><em>balance-tlb</em> (modo 5): balancea la carga de transmisión entre los esclavos dependiendo de la velocidad de estos y de la carga total. El tráfico es recibido por un esclavo, en caso de fallar otro esclavo toma su <em>MAC</em> y continúa recibiendo tráfico.</li>
<li><em>balance-alb</em> (modo 6): realiza el balanceo anterior además de un balanceo también en la recepción. Este método debe modificar las <em>MAC</em> de los esclavos estando las tarjetas activas, esto debe estar soportado por el driver para poder usar este método.</li></ul>

Aunque todo esto parezca muy bonito, hay que tener en cuenta una cosa: algunos métodos necesitan ciertas configuraciones/capacidades en el <em>switch</em> al que estés conectados los esclavos. Ah! se me olvidaba: si el switch tiene un único <em>link</em> de salida a la red y va a la misma velocidad que nuestros esclavos, el balanceo se hace inútil ya que se produce un cuello de botella en el <em>link</em> de salida del <em>switch</em>.

<h3>Ahora si: carga y configuración del módulo <em>bonding</em></h3>
Cargaremos el módulo como de costumbre:


<div class="console">
{% highlight console %}
# modprobe bonding [parámetros]
{% endhighlight %}
</div>

Y los parámetros los tenemos a continuación:

<ul><li><em>arp_interval</em>: si se especifica (por defecto es 0, desactivado) indica cada cuántos milisegundos se enviará un <em>ARP reply</em>. Este tipo de detección es compatible con las políticas 0 y 2.</li>
<li><em>arp_ip_target</em>: indica cuál será la IP destino. Se pueden especificar varias separadas por comas (como máximo 16).</li>
<li><em>downdelay</em>: especifica cuántos milisegundos se tardará en desactivar un esclavo cuando se detecte un error en el enlace. Por defecto es 0 (inmediatamente) y sólo se emplea en la detección mediante <em>MII</em>.</li>
<li><em>lacp_rate</em>: en el modo <em>802.3ad</em> especifica cada cuánto se va a envíar un mensaje <em>LACDPU</em>. Estos paquetes se encargan de controlar el protocolo de agregación de enlaces. Este parámetro tiene dos posibles valores: <em>slow</em> o 0 (por defecto) y <em>fast</em> o 1.</li>
<li><em>max_bonds</em>: indica cuántas interfaces virtuales creará como máximo esta instancia del módulo. Por defecto vale 1 (sólo <em>bond0</em>).</li>
<li><em>miimon</em>: si especificamos un valor distinto de 0 (por defecto) activa la detección por <em>MII</em>, indica cada cuántos milisegundos se va a comprobar el estado del enlace. Un buen valor puede ser 100.</li>
<li><em>mode</em>: indica que algoritmo de balanceo se va a usar, sus posibles valores se indicaron en el apartado anterior.</li>
<li><em>primary</em>: este parámetro sólo se usa si se activa el modo 1, y se usa para indicar cuál esclavo es el primario.</li>
<li><em>updelay</em>: especifica cuánto  tiempo en milisegundos se tardará en activar un esclavo cuando se detecta un enlace repuesto. Por defecto vale 0.</li>
<li><em>use_carrier</em>: por defecto vale 1, lo cual indica que se usará una llamada a netif_carrier_ok() del módulo de la NIC para la detección por <em>MII</em>. Si vale 0 se usarán llamadas <em>ETHTOOL ioctl's</em>, lo cual está <em>deprecated</em>.</li>
<li><em>xmit_hash_policy</em>: especifica la política de transmisión para los modos 2 y 4. Los posibles valores son:
<ul><li><em>layer2</em>: se usa la fórmula que se indicó en el modo 2 (apartado anterior). Este algoritmo hace que el tráfico con un mismo destino "salga" siempre por la misma interfaz. Es la política usada por defecto.</li>
<li><em>layer3+4</em>: con esta política se usará información de niveles superiores al de enlace (realiza XOR's también con las IP's) y no tiene el mismo comportamiento según la clase de tráfico transportado. Es un buen algoritmo pero no es 100% compatible con <em>IEEE 802.3ad</em>.</li></ul></li>

Con tantos parámetros podremos probar muchas opciones, yo actualmente uso:


<div class="console">
{% highlight console %}
# modprobe bonding miimon=100 mode=balance-tlb
{% endhighlight %}
</div>

Con esto ya tenemos el módulo cargado, sólo nos falta configurar la interfaz y añadir esclavos.

<h2>Configurar interfaz <em>bond</em></h2>
Si véis <em>how-to's</em> por internet veréis que montan un "pitoste" de parámetros en ficheros, etc. Pero es porque usan SuSE y cosas de esas raras, encima para tener IP estática. Pues en Debian abrimos <tt>/etc/networking/interfaces</tt> y añadimos:

<pre>iface bond0 inet dhcp
    slaves eth0 eth1</pre>

Tened en cuenta que <em>eth0</em> y <em>eth1</em> no deben estar activos en ese momento (deben estar <em>down</em>). Y finalmente para activar la interfaz:


<div class="console">
{% highlight console %}
# ifup bond0
{% endhighlight %}
</div>

Y ale... a disfrutar... ;)

<h2>Apéndice A: Configuración del <em>switch</em></h2>
Los modos <em>active-backup</em>, <em>balance-tlb</em> y <em>balance-alb</em> no requiere ninguna configuración especial en el <em>switch</em>, ideales si no tenemos acceso a la configuración del equipamiento de red ;-).

El modo <em>802.3ad</em> requiere que el <em>switch</em> tenga los puertos donde conectamos los esclavos en modo <em>802.3ad aggregation</em>. Esto depende de cada <em>switch</em>, por ejemplo, en los <em>switch's</em> de Cisco esta capacidad se llama <em>EtherChannel</em> y debe estar en modo <em>lacp</em>.

Por último, los modos <em>balance-rr</em>, <em>balance-xor</em> y <em>broadcast</em> generalmente requiere poder agrupar puertos. Las nomenclaturas de estos grupos dependen del fabricante del <em>switch</em>, como hemos dicho antes, Cisco llama a estas agrupaciones <em>EtherChannel</em>, también se usa <em>trunk group</em>, etc.

<h2>Enlaces de interés</h2>
Pues hay mucho por internet, pero lo mejor es la documentación del propio módulo:

<a href="http://www.cyberciti.biz/howto/question/static/linux-ethernet-bonding-driver-howto.php">Linux Ethernet Bondign Driver HOW-TO</a>.

Si probáis la receta, podéis envíar vuestras configuraciones (tanto del <em>switch</em> si la conocéis y de vuestro <em>bond</em> para hacer comparativas de rendimientos y todo eso. :-)
